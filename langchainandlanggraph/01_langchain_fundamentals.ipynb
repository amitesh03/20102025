{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Fundamentals - Interactive Notebook\n",
    "\n",
    "This notebook provides hands-on examples for the concepts covered in the LangChain fundamentals lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Core LangChain imports\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.chains import LLMChain\n",
    "from langchain_core.memory import ConversationBufferMemory\n",
    "\n",
    "# Model imports\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "\n",
    "print(\"Successfully imported LangChain components!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI models\n",
    "openai_llm = OpenAI(temperature=0.7)\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "print(\"Models initialized successfully!\")\n",
    "print(f\"OpenAI LLM: {openai_llm}\")\n",
    "print(f\"Chat Model: {chat_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple prompt template\n",
    "simple_template = \"Tell me a joke about {topic}.\"\n",
    "simple_prompt = PromptTemplate(\n",
    "    template=simple_template,\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = simple_prompt.format(topic=\"programming\")\n",
    "print(\"Formatted Prompt:\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chain\n",
    "joke_chain = LLMChain(\n",
    "    llm=openai_llm,\n",
    "    prompt=simple_prompt\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = joke_chain.run(topic=\"programming\")\n",
    "print(\"Generated Joke:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Working with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a memory component\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Add some conversation\n",
    "memory.chat_memory.add_user_message(\"Hi, I'm learning LangChain!\")\n",
    "memory.chat_memory.add_ai_message(\"That's great! LangChain is a powerful framework for building LLM applications.\")\n",
    "memory.chat_memory.add_user_message(\"What are the main components?\")\n",
    "memory.chat_memory.add_ai_message(\"The main components include models, prompts, chains, memory, and agents.\")\n",
    "\n",
    "print(\"Conversation History:\")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating a Chain with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversational prompt template\n",
    "conversational_template = \"\"\"\n",
    "You are a helpful assistant. Here's the conversation so far:\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "conversational_prompt = PromptTemplate(\n",
    "    template=conversational_template,\n",
    "    input_variables=[\"chat_history\", \"human_input\"]\n",
    ")\n",
    "\n",
    "# Create a chain with memory\n",
    "conversational_chain = LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=conversational_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create a function to handle conversation\n",
    "def chat_with_assistant(user_input, memory):\n",
    "    # Get chat history\n",
    "    chat_history = memory.buffer\n",
    "    \n",
    "    # Generate response\n",
    "    response = conversational_chain.run(\n",
    "        chat_history=chat_history,\n",
    "        human_input=user_input\n",
    "    )\n",
    "    \n",
    "    # Update memory\n",
    "    memory.chat_memory.add_user_message(user_input)\n",
    "    memory.chat_memory.add_ai_message(response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the conversation\n",
    "response1 = chat_with_assistant(\"What can you tell me about LangChain?\", memory)\n",
    "print(\"Assistant:\", response1)\n",
    "\n",
    "response2 = chat_with_assistant(\"Can you give me a simple example?\", memory)\n",
    "print(\"Assistant:\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise: Story Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a story generator chain\n",
    "# 1. Create a prompt template for story generation\n",
    "# 2. Initialize a chain\n",
    "# 3. Add memory to remember previous stories\n",
    "# 4. Test the chain with different topics\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# Story template\n",
    "story_template = \"\"\"\n",
    "Write a short story about {topic}. The story should be engaging and creative.\n",
    "Previous stories you've written: {previous_stories}\n",
    "\n",
    "Story:\"\"\"\n",
    "\n",
    "# Create prompt\n",
    "story_prompt = PromptTemplate(\n",
    "    template=story_template,\n",
    "    input_variables=[\"topic\", \"previous_stories\"]\n",
    ")\n",
    "\n",
    "# Create chain\n",
    "story_chain = LLMChain(\n",
    "    llm=openai_llm,\n",
    "    prompt=story_prompt\n",
    ")\n",
    "\n",
    "# Memory for stories\n",
    "story_memory = ConversationBufferMemory()\n",
    "\n",
    "# Function to generate stories\n",
    "def generate_story(topic):\n",
    "    previous_stories = story_memory.buffer if story_memory.buffer else \"None yet\"\n",
    "    \n",
    "    story = story_chain.run(\n",
    "        topic=topic,\n",
    "        previous_stories=previous_stories\n",
    "    )\n",
    "    \n",
    "    story_memory.chat_memory.add_user_message(f\"Topic: {topic}\")\n",
    "    story_memory.chat_memory.add_ai_message(story)\n",
    "    \n",
    "    return story\n",
    "\n",
    "# Test the story generator\n",
    "story1 = generate_story(\"space exploration\")\n",
    "print(\"Story 1:\")\n",
    "print(story1)\n",
    "\n",
    "story2 = generate_story(\"artificial intelligence\")\n",
    "print(\"\\nStory 2:\")\n",
    "print(story2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "- Setting up LangChain environment\n",
    "- Initializing different types of models\n",
    "- Creating and using prompt templates\n",
    "- Building simple chains\n",
    "- Implementing memory for persistent conversations\n",
    "- Creating a practical story generator application\n",
    "\n",
    "These fundamentals form the building blocks for more complex LangChain applications that we'll explore in upcoming lessons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}