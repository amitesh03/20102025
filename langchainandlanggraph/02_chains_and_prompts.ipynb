{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chains and Prompt Templates - Interactive Notebook\n",
    "\n",
    "This notebook provides hands-on examples for implementing chains and prompt templates in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Core LangChain imports\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "from langchain_core.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain_core.chains import LLMChain, SequentialChain, TransformChain\n",
    "from langchain_core.memory import ConversationBufferMemory\n",
    "\n",
    "# Model imports\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "\n",
    "print(\"Successfully imported LangChain components!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working with Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different types of prompt templates\n",
    "\n",
    "# Simple template\n",
    "simple_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# Multi-variable template\n",
    "multi_var_template = PromptTemplate(\n",
    "    template=\"Translate the following text from {source_lang} to {target_lang}: {text}\",\n",
    "    input_variables=[\"source_lang\", \"target_lang\", \"text\"]\n",
    ")\n",
    "\n",
    "# Template with partial variables\n",
    "partial_template = PromptTemplate(\n",
    "    template=\"As a {role}, explain {topic} in a {tone} manner.\",\n",
    "    input_variables=[\"topic\", \"tone\"],\n",
    "    partial_variables={\"role\": \"expert educator\"}\n",
    ")\n",
    "\n",
    "print(\"Templates created successfully!\")\n",
    "\n",
    "# Format and print examples\n",
    "print(\"Simple template:\", simple_template.format(country=\"Japan\"))\n",
    "print(\"\\nMulti-variable template:\", multi_var_template.format(\n",
    "    source_lang=\"English\", \n",
    "    target_lang=\"Spanish\", \n",
    "    text=\"Hello, how are you?\"\n",
    "))\n",
    "print(\"\\nPartial template:\", partial_template.format(topic=\"quantum physics\", tone=\"simple\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Few-Shot Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create few-shot examples\n",
    "examples = [\n",
    "    {\"question\": \"What is 2+2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"What is 5*3?\", \"answer\": \"15\"},\n",
    "    {\"question\": \"What is 10-7?\", \"answer\": \"3\"},\n",
    "    {\"question\": \"What is 8/2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"What is 6+9?\", \"answer\": \"15\"}\n",
    "]\n",
    "\n",
    "# Create example prompt\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"Question: {question}\\nAnswer: {answer}\"\n",
    ")\n",
    "\n",
    "# Create few-shot prompt template\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples[:3],  # Use first 3 examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Here are some examples of math problems:\",\n",
    "    suffix=\"Question: {input}\\nAnswer:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "print(\"Few-shot prompt template created!\")\n",
    "print(\"\\nFormatted prompt:\")\n",
    "print(few_shot_prompt.format(input=\"What is 7*8?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamic Example Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example selector for dynamic examples\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100  # Maximum length of examples\n",
    ")\n",
    "\n",
    "# Create dynamic prompt\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Here are some examples:\",\n",
    "    suffix=\"Question: {input}\\nAnswer:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "print(\"Dynamic prompt with example selector created!\")\n",
    "print(\"\\nFormatted dynamic prompt:\")\n",
    "print(dynamic_prompt.format(input=\"What is 12*15?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chat Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat prompt template\n",
    "system_template = \"You are a helpful assistant that specializes in {subject}.\"\n",
    "human_template = \"{user_input}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "\n",
    "# Format the chat prompt\n",
    "formatted_messages = chat_prompt.format_messages(\n",
    "    subject=\"history\",\n",
    "    user_input=\"Tell me about the Renaissance period.\"\n",
    ")\n",
    "\n",
    "print(\"Chat prompt template created!\")\n",
    "print(\"\\nFormatted messages:\")\n",
    "for message in formatted_messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simple LLM Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Create a simple explanation chain\n",
    "explanation_prompt = PromptTemplate(\n",
    "    template=\"Explain {concept} in simple terms.\",\n",
    "    input_variables=[\"concept\"]\n",
    ")\n",
    "\n",
    "explanation_chain = LLMChain(llm=llm, prompt=explanation_prompt, verbose=True)\n",
    "\n",
    "# Test the chain\n",
    "result = explanation_chain.run(concept=\"quantum computing\")\n",
    "print(\"\\nExplanation:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple chains for sequential execution\n",
    "\n",
    "# Chain 1: Generate a topic\n",
    "topic_prompt = PromptTemplate(\n",
    "    template=\"Generate an interesting topic about {subject}.\",\n",
    "    input_variables=[\"subject\"]\n",
    ")\n",
    "topic_chain = LLMChain(llm=llm, prompt=topic_prompt, output_key=\"topic\")\n",
    "\n",
    "# Chain 2: Create content about the topic\n",
    "content_prompt = PromptTemplate(\n",
    "    template=\"Write a short paragraph about {topic}.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "content_chain = LLMChain(llm=llm, prompt=content_prompt, output_key=\"content\")\n",
    "\n",
    "# Chain 3: Summarize the content\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Summarize this content in one sentence: {content}\",\n",
    "    input_variables=[\"content\"]\n",
    ")\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt, output_key=\"summary\")\n",
    "\n",
    "# Combine into a sequential chain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[topic_chain, content_chain, summary_chain],\n",
    "    input_variables=[\"subject\"],\n",
    "    output_variables=[\"topic\", \"content\", \"summary\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the sequential chain\n",
    "result = overall_chain({\"subject\": \"artificial intelligence\"})\n",
    "print(\"\\nSequential Chain Results:\")\n",
    "print(f\"Topic: {result['topic']}\")\n",
    "print(f\"Content: {result['content']}\")\n",
    "print(f\"Summary: {result['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Transform Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transform chain to process input\n",
    "def transform_func(inputs):\n",
    "    \"\"\"Transform input data\"\"\"\n",
    "    text = inputs[\"text\"]\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    char_count = len(text)\n",
    "    \n",
    "    return {\n",
    "        \"original_text\": text,\n",
    "        \"word_count\": word_count,\n",
    "        \"char_count\": char_count,\n",
    "        \"analysis\": f\"The text has {word_count} words and {char_count} characters.\"\n",
    "    }\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"original_text\", \"word_count\", \"char_count\", \"analysis\"],\n",
    "    transform=transform_func\n",
    ")\n",
    "\n",
    "# Create a summary chain\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Analyze this text: {original_text}\\n\\n{analysis}\\n\\nProvide insights:\",\n",
    "    input_variables=[\"original_text\", \"analysis\"]\n",
    ")\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt, output_key=\"insights\")\n",
    "\n",
    "# Combine chains\n",
    "full_chain = SequentialChain(\n",
    "    chains=[transform_chain, summary_chain],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"analysis\", \"word_count\", \"char_count\", \"insights\"]\n",
    ")\n",
    "\n",
    "# Test the transform + LLM chain\n",
    "result = full_chain.run(text=\"LangChain is a powerful framework for building applications with large language models.\")\n",
    "print(\"\\nTransform Chain Results:\")\n",
    "print(f\"Analysis: {result['analysis']}\")\n",
    "print(f\"Word Count: {result['word_count']}\")\n",
    "print(f\"Character Count: {result['char_count']}\")\n",
    "print(f\"Insights: {result['insights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Chain with Memory Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain with integrated memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"input\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant. Here's your conversation history:\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Human: {input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"input\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "memory_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the memory chain\n",
    "response1 = memory_chain.run(input=\"Hi, I'm learning about LangChain.\")\n",
    "print(f\"\\nAssistant 1: {response1}\")\n",
    "\n",
    "response2 = memory_chain.run(input=\"Can you explain what chains are?\")\n",
    "print(f\"\\nAssistant 2: {response2}\")\n",
    "\n",
    "response3 = memory_chain.run(input=\"How do I create a sequential chain?\")\n",
    "print(f\"\\nAssistant 3: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercise: Multi-Step Content Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a multi-step content generator\n",
    "# 1. Create prompts for each step\n",
    "# 2. Create chains for each step\n",
    "# 3. Combine them into a sequential chain\n",
    "# 4. Test with different topics\n",
    "\n",
    "# Step 1: Title generation\n",
    "title_prompt = PromptTemplate(\n",
    "    template=\"Generate a catchy blog post title about {topic}.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "title_chain = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n",
    "\n",
    "# Step 2: Outline creation\n",
    "outline_prompt = PromptTemplate(\n",
    "    template=\"Create a 3-point outline for a blog post titled '{title}'.\",\n",
    "    input_variables=[\"title\"]\n",
    ")\n",
    "outline_chain = LLMChain(llm=llm, prompt=outline_prompt, output_key=\"outline\")\n",
    "\n",
    "# Step 3: Introduction\n",
    "intro_prompt = PromptTemplate(\n",
    "    template=\"Write an engaging introduction for a blog post titled '{title}' with this outline: {outline}\",\n",
    "    input_variables=[\"title\", \"outline\"]\n",
    ")\n",
    "intro_chain = LLMChain(llm=llm, prompt=intro_prompt, output_key=\"introduction\")\n",
    "\n",
    "# Step 4: Key points\n",
    "key_points_prompt = PromptTemplate(\n",
    "    template=\"Expand on these key points from the outline: {outline}\",\n",
    "    input_variables=[\"outline\"]\n",
    ")\n",
    "key_points_chain = LLMChain(llm=llm, prompt=key_points_prompt, output_key=\"key_points\")\n",
    "\n",
    "# Step 5: Conclusion\n",
    "conclusion_prompt = PromptTemplate(\n",
    "    template=\"Write a strong conclusion for a blog post with this introduction: {introduction}\",\n",
    "    input_variables=[\"introduction\"]\n",
    ")\n",
    "conclusion_chain = LLMChain(llm=llm, prompt=conclusion_prompt, output_key=\"conclusion\")\n",
    "\n",
    "# Combine all chains\n",
    "content_generator = SequentialChain(\n",
    "    chains=[title_chain, outline_chain, intro_chain, key_points_chain, conclusion_chain],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"title\", \"outline\", \"introduction\", \"key_points\", \"conclusion\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the content generator\n",
    "result = content_generator.run(topic=\"sustainable living\")\n",
    "print(\"\\n=== CONTENT GENERATOR RESULTS ===\")\n",
    "print(f\"Title: {result['title']}\")\n",
    "print(f\"\\nOutline: {result['outline']}\")\n",
    "print(f\"\\nIntroduction: {result['introduction']}\")\n",
    "print(f\"\\nKey Points: {result['key_points']}\")\n",
    "print(f\"\\nConclusion: {result['conclusion']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "- Creating and customizing various types of prompt templates\n",
    "- Implementing few-shot learning with examples\n",
    "- Building simple and sequential chains\n",
    "- Creating transform chains for data processing\n",
    "- Integrating memory into chains\n",
    "- Building a multi-step content generator\n",
    "\n",
    "These techniques form the foundation for creating sophisticated LangChain applications that can handle complex workflows and maintain context across interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}