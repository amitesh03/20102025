{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LLM Integration - Interactive Notebook\n",
    "\n",
    "This notebook provides hands-on examples for integrating and working with different LLM models in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "from functools import lru_cache\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Core LangChain imports\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Model imports\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceHub, HuggingFaceEndpoint\n",
    "\n",
    "# Token counting\n",
    "import tiktoken\n",
    "\n",
    "print(\"Successfully imported all libraries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working with Different Model Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize different types of models\n",
    "\n",
    "# OpenAI completion model\n",
    "openai_completion = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# OpenAI chat model\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(\"Models initialized successfully!\")\n",
    "\n",
    "# Test completion model\n",
    "completion_response = openai_completion(\"Write a haiku about technology:\")\n",
    "print(\"Completion Model Response:\")\n",
    "print(completion_response)\n",
    "\n",
    "# Test chat model\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Write a haiku about technology.\")\n",
    "]\n",
    "chat_response = chat_model.invoke(messages)\n",
    "print(\"\\nChat Model Response:\")\n",
    "print(chat_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Parameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different temperature settings\n",
    "low_temp_model = OpenAI(temperature=0.1, max_tokens=100)\n",
    "high_temp_model = OpenAI(temperature=0.9, max_tokens=100)\n",
    "\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(\"Low temperature (0.1) - More focused:\")\n",
    "low_response = low_temp_model(prompt)\n",
    "print(low_response)\n",
    "\n",
    "print(\"\\nHigh temperature (0.9) - More creative:\")\n",
    "high_response = high_temp_model(prompt)\n",
    "print(high_response)\n",
    "\n",
    "# Test other parameters\n",
    "advanced_model = OpenAI(\n",
    "    temperature=0.7,\n",
    "    max_tokens=50,\n",
    "    stop=[\"\\n\\n\", \"###\"],  # Stop at these sequences\n",
    "    frequency_penalty=0.5,  # Reduce repetition\n",
    "    presence_penalty=0.3   # Encourage new topics\n",
    ")\n",
    "\n",
    "print(\"\\nAdvanced model with stop sequences:\")\n",
    "advanced_response = advanced_model(\"List three benefits of renewable energy:\")\n",
    "print(advanced_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Different Model Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different OpenAI models\n",
    "models = {\n",
    "    \"gpt-3.5-turbo\": ChatOpenAI(model=\"gpt-3.5-turbo\", max_tokens=50),\n",
    "    \"gpt-3.5-turbo-instruct\": OpenAI(model=\"gpt-3.5-turbo-instruct\", max_tokens=50)\n",
    "}\n",
    "\n",
    "test_prompt = \"Explain machine learning in one sentence.\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        if hasattr(model, 'invoke'):\n",
    "            response = model.invoke(test_prompt)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "        else:\n",
    "            content = model(test_prompt)\n",
    "        print(f\"{name}: {content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: Error - {e}\")\n",
    "\n",
    "# Test Hugging Face model (if token is available)\n",
    "try:\n",
    "    hf_model = HuggingFaceHub(\n",
    "        repo_id=\"google/flan-t5-large\",\n",
    "        model_kwargs={\"temperature\": 0.5, \"max_length\": 100}\n",
    "    )\n",
    "    \n",
    "    hf_response = hf_model(\"Translate to French: 'Hello, how are you?'\")\n",
    "    print(f\"\\nHugging Face response: {hf_response}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nHugging Face model not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming model\n",
    "streaming_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    streaming=True,\n",
    "    temperature=0.7,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in streaming_model.stream(\"Tell me a short story about a robot learning to paint:\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()\n",
    "\n",
    "# Streaming with callbacks\n",
    "print(\"\\nStreaming with callbacks:\")\n",
    "callback_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "response = callback_model.invoke(\"Explain the concept of neural networks in simple terms:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Token Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token counting function\n",
    "def count_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Test token counting\n",
    "test_text = \"This is a sample text to count tokens.\"\n",
    "token_count = count_tokens(test_text)\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Token count: {token_count}\")\n",
    "\n",
    "# Optimize prompts for token usage\n",
    "def optimize_prompt(prompt, max_tokens=100):\n",
    "    token_count = count_tokens(prompt)\n",
    "    if token_count > max_tokens:\n",
    "        # Truncate the prompt\n",
    "        words = prompt.split()\n",
    "        truncated = \" \".join(words[:int(len(words) * max_tokens / token_count)])\n",
    "        return truncated\n",
    "    return prompt\n",
    "\n",
    "# Test prompt optimization\n",
    "long_prompt = \"This is a very long prompt that contains a lot of text and details about various topics \" * 10\n",
    "print(f\"\\nOriginal prompt length: {len(long_prompt)} characters\")\n",
    "print(f\"Original token count: {count_tokens(long_prompt)}\")\n",
    "\n",
    "optimized = optimize_prompt(long_prompt, max_tokens=50)\n",
    "print(f\"\\nOptimized prompt length: {len(optimized)} characters\")\n",
    "print(f\"Optimized token count: {count_tokens(optimized)}\")\n",
    "print(f\"Optimized prompt: {optimized[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Response Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement caching for LLM responses\n",
    "class CachedLLM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.cache = {}\n",
    "    \n",
    "    def _get_cache_key(self, prompt, **kwargs):\n",
    "        # Create a unique key for the prompt and parameters\n",
    "        key_data = {\"prompt\": prompt, **kwargs}\n",
    "        return hashlib.md5(json.dumps(key_data, sort_keys=True).encode()).hexdigest()\n",
    "    \n",
    "    def invoke(self, prompt, **kwargs):\n",
    "        cache_key = self._get_cache_key(prompt, **kwargs)\n",
    "        \n",
    "        if cache_key in self.cache:\n",
    "            print(\"Using cached response\")\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        print(\"Generating new response\")\n",
    "        response = self.model.invoke(prompt, **kwargs)\n",
    "        content = response.content if hasattr(response, 'content') else str(response)\n",
    "        self.cache[cache_key] = content\n",
    "        return content\n",
    "\n",
    "# Test cached LLM\n",
    "cached_model = CachedLLM(ChatOpenAI(model=\"gpt-3.5-turbo\", max_tokens=50))\n",
    "\n",
    "# First call (generates new response)\n",
    "print(\"First call:\")\n",
    "response1 = cached_model.invoke(\"What is machine learning?\")\n",
    "print(f\"Response: {response1}\")\n",
    "\n",
    "# Second call (uses cached response)\n",
    "print(\"\\nSecond call:\")\n",
    "response2 = cached_model.invoke(\"What is machine learning?\")\n",
    "print(f\"Response: {response2}\")\n",
    "\n",
    "# Different prompt (generates new response)\n",
    "print(\"\\nDifferent prompt:\")\n",
    "response3 = cached_model.invoke(\"What is deep learning?\")\n",
    "print(f\"Response: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Handling and Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement robust error handling\n",
    "class RobustLLM:\n",
    "    def __init__(self, model, max_retries=3, retry_delay=1):\n",
    "        self.model = model\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "    \n",
    "    def invoke(self, prompt, **kwargs) -> Optional[str]:\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.model.invoke(prompt, **kwargs)\n",
    "                return response.content if hasattr(response, 'content') else str(response)\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (2 ** attempt))  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"Max retries exceeded for prompt: {prompt[:50]}...\")\n",
    "                    return None\n",
    "        return None\n",
    "\n",
    "# Test robust LLM\n",
    "robust_model = RobustLLM(ChatOpenAI(model=\"gpt-3.5-turbo\", max_tokens=50))\n",
    "\n",
    "response = robust_model.invoke(\"Explain the concept of blockchain in simple terms:\")\n",
    "if response:\n",
    "    print(f\"Response: {response}\")\n",
    "else:\n",
    "    print(\"Failed to get response after retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement performance monitoring\n",
    "class MonitoredLLM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"total_time\": 0,\n",
    "            \"errors\": 0\n",
    "        }\n",
    "    \n",
    "    def invoke(self, prompt, **kwargs) -> Dict[str, Any]:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = self.model.invoke(prompt, **kwargs)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            response_time = end_time - start_time\n",
    "            input_tokens = count_tokens(prompt)\n",
    "            output_content = response.content if hasattr(response, 'content') else str(response)\n",
    "            output_tokens = count_tokens(output_content)\n",
    "            \n",
    "            # Update stats\n",
    "            self.stats[\"total_requests\"] += 1\n",
    "            self.stats[\"total_tokens\"] += input_tokens + output_tokens\n",
    "            self.stats[\"total_time\"] += response_time\n",
    "            \n",
    "            return {\n",
    "                \"content\": output_content,\n",
    "                \"response_time\": response_time,\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"total_tokens\": input_tokens + output_tokens\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.stats[\"errors\"] += 1\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"response_time\": time.time() - start_time\n",
    "            }\n",
    "    \n",
    "    def get_stats(self):\n",
    "        avg_time = self.stats[\"total_time\"] / max(1, self.stats[\"total_requests\"])\n",
    "        avg_tokens = self.stats[\"total_tokens\"] / max(1, self.stats[\"total_requests\"])\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"average_response_time\": avg_time,\n",
    "            \"average_tokens_per_request\": avg_tokens\n",
    "        }\n",
    "\n",
    "# Test monitored LLM\n",
    "monitored_model = MonitoredLLM(ChatOpenAI(model=\"gpt-3.5-turbo\", max_tokens=100))\n",
    "\n",
    "# Make several requests\n",
    "prompts = [\n",
    "    \"Write a haiku about technology.\",\n",
    "    \"Explain photosynthesis in one sentence.\",\n",
    "    \"What is the capital of France?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = monitored_model.invoke(prompt)\n",
    "    if \"content\" in result:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {result['content'][:50]}...\")\n",
    "        print(f\"Tokens: {result['total_tokens']}, Time: {result['response_time']:.2f}s\\n\")\n",
    "\n",
    "# Print statistics\n",
    "stats = monitored_model.get_stats()\n",
    "print(\"Model Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercise: Smart LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a smart LLM wrapper\n",
    "# 1. Create task complexity detection\n",
    "# 2. Implement model selection logic\n",
    "# 3. Add caching layer\n",
    "# 4. Include fallback mechanisms\n",
    "# 5. Add performance monitoring\n",
    "\n",
    "class SmartLLM:\n",
    "    def __init__(self):\n",
    "        # Initialize different models for different tasks\n",
    "        self.simple_model = ChatOpenAI(model=\"gpt-3.5-turbo\", max_tokens=150)\n",
    "        self.complex_model = ChatOpenAI(model=\"gpt-4\", max_tokens=300)  # If available\n",
    "        \n",
    "        # Initialize cache\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Task complexity keywords\n",
    "        self.complex_keywords = [\n",
    "            \"analyze\", \"compare\", \"synthesize\", \"evaluate\", \"create\",\n",
    "            \"complex\", \"detailed\", \"comprehensive\", \"in-depth\"\n",
    "        ]\n",
    "    \n",
    "    def _detect_complexity(self, prompt: str) -> bool:\n",
    "        \"\"\"Determine if the task is complex based on keywords and length\"\"\"\n",
    "        # Check for complexity keywords\n",
    "        has_complex_keywords = any(keyword in prompt.lower() for keyword in self.complex_keywords)\n",
    "        \n",
    "        # Check prompt length (longer prompts tend to be more complex)\n",
    "        is_long_prompt = len(prompt.split()) > 50\n",
    "        \n",
    "        return has_complex_keywords or is_long_prompt\n",
    "    \n",
    "    def _get_cache_key(self, prompt: str) -> str:\n",
    "        \"\"\"Generate cache key for prompt\"\"\"\n",
    "        return hashlib.md5(prompt.encode()).hexdigest()\n",
    "    \n",
    "    def invoke(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Smart invocation with model selection, caching, and fallback\"\"\"\n",
    "        cache_key = self._get_cache_key(prompt)\n",
    "        \n",
    "        # Check cache first\n",
    "        if cache_key in self.cache:\n",
    "            print(\"Using cached response\")\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Select appropriate model\n",
    "        is_complex = self._detect_complexity(prompt)\n",
    "        selected_model = self.complex_model if is_complex else self.simple_model\n",
    "        \n",
    "        print(f\"Using {'complex' if is_complex else 'simple'} model\")\n",
    "        \n",
    "        try:\n",
    "            # Get response\n",
    "            response = selected_model.invoke(prompt, **kwargs)\n",
    "            content = response.content if hasattr(response, 'content') else str(response)\n",
    "            \n",
    "            # Cache the response\n",
    "            self.cache[cache_key] = content\n",
    "            \n",
    "            return content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Selected model failed: {e}\")\n",
    "            \n",
    "            # Try fallback model\n",
    "            try:\n",
    "                fallback_model = self.simple_model if is_complex else self.complex_model\n",
    "                print(\"Trying fallback model\")\n",
    "                \n",
    "                response = fallback_model.invoke(prompt, **kwargs)\n",
    "                content = response.content if hasattr(response, 'content') else str(response)\n",
    "                \n",
    "                self.cache[cache_key] = content\n",
    "                return content\n",
    "                \n",
    "            except Exception as fallback_error:\n",
    "                print(f\"Fallback also failed: {fallback_error}\")\n",
    "                return \"I'm unable to process your request at the moment.\"\n",
    "\n",
    "# Test the smart LLM\n",
    "smart_llm = SmartLLM()\n",
    "\n",
    "# Test simple task\n",
    "print(\"=== Testing Simple Task ===\")\n",
    "simple_response = smart_llm.invoke(\"What is the capital of France?\")\n",
    "print(f\"Response: {simple_response}\\n\")\n",
    "\n",
    "# Test complex task\n",
    "print(\"=== Testing Complex Task ===\")\n",
    "complex_response = smart_llm.invoke(\"Analyze the impact of artificial intelligence on global economics and society.\")\n",
    "print(f\"Response: {complex_response[:200]}...\\n\")\n",
    "\n",
    "# Test caching (should use cached response)\n",
    "print(\"=== Testing Caching ===\")\n",
    "cached_response = smart_llm.invoke(\"What is the capital of France?\")\n",
    "print(f\"Cached response: {cached_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "- Working with different types of LLM models (completion vs chat)\n",
    "- Configuring model parameters for optimal performance\n",
    "- Implementing streaming responses for better user experience\n",
    "- Managing tokens and optimizing prompts\n",
    "- Adding caching to improve performance and reduce costs\n",
    "- Implementing robust error handling and retry logic\n",
    "- Monitoring model performance metrics\n",
    "- Building a smart LLM wrapper with automatic model selection\n",
    "\n",
    "These techniques provide a solid foundation for building robust and efficient LLM-powered applications with LangChain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}