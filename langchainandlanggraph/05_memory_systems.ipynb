{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Systems in LangChain - Interactive Notebook\n",
    "\n",
    "This notebook implements hands-on examples for:\n",
    "- Conversation buffer memory\n",
    "- Summary memory\n",
    "- Vector memory with Chroma\n",
    "- Lightweight knowledge graph memory (triple store)\n",
    "- Persistence patterns (JSON + vector store persistence)\n",
    "- Capstone: Memory-enabled assistant\n",
    "\n",
    "Note: Some cells use OpenAI (for embeddings or LLM). Ensure your `.env` is configured (OPENAI_API_KEY)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.chains import LLMChain\n",
    "from langchain_core.memory import ConversationBufferMemory\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "print(\"Environment and imports set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Conversation Buffer Memory\n",
    "ConversationBufferMemory keeps verbatim chat history and injects it into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.4)\n",
    "except Exception as e:\n",
    "    llm = None\n",
    "    print(\"ChatOpenAI unavailable (check OPENAI_API_KEY):\", e)\n",
    "\n",
    "prompt_text = \"\"\"You are a helpful assistant.\n",
    "{chat_history}\n",
    "Human: {input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_text,\n",
    "    input_variables=[\"input\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"input\")\n",
    "\n",
    "if llm:\n",
    "    chat_chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=True)\n",
    "    try:\n",
    "        print(chat_chain.run(input=\"Hi, I'm learning memory systems.\"))\n",
    "        print(chat_chain.run(input=\"What did I say earlier?\"))\n",
    "    except Exception as e:\n",
    "        print(\"Chain run failed:\", e)\n",
    "else:\n",
    "    print(\"Skipping LLM calls; no API key configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise A:\n",
    "- Modify `prompt_text` to include a system rule like \"Always answer concisely\".\n",
    "- Observe differences across several turns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Summary Memory Strategy\n",
    "Summarize previous turns to compress context and stay within token limits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    summarizer_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "except Exception as e:\n",
    "    summarizer_llm = None\n",
    "    print(\"Summarizer model unavailable:\", e)\n",
    "\n",
    "summarizer_prompt = PromptTemplate(\n",
    "    template=\"\"\"Summarize the following conversation in 2-3 sentences.\n",
    "{conversation}\n",
    "Summary:\"\"\",\n",
    "    input_variables=[\"conversation\"]\n",
    ")\n",
    "\n",
    "conversation_log: List[str] = []\n",
    "\n",
    "def append_and_summarize(user_text: str, ai_text: Optional[str] = None) -> str:\n",
    "    conversation_log.append(f\"Human: {user_text}\")\n",
    "    if ai_text is not None:\n",
    "        conversation_log.append(f\"Assistant: {ai_text}\")\n",
    "    joined = \"\\n\".join(conversation_log[-20:])\n",
    "    if summarizer_llm:\n",
    "        chain = LLMChain(llm=summarizer_llm, prompt=summarizer_prompt)\n",
    "        try:\n",
    "            return chain.run(conversation=joined)\n",
    "        except Exception as e:\n",
    "            return f\"Summarization failed: {e}\"\n",
    "    return \"Summarizer not available (no API key).\"\n",
    "\n",
    "# Demo (mock AI text to illustrate summarization without consuming tokens)\n",
    "s1 = append_and_summarize(\"Plan a 3-day trip to Goa.\", \"Day-by-day itinerary...\")\n",
    "print(\"Summary 1:\\n\", s1)\n",
    "s2 = append_and_summarize(\"Make it budget-friendly.\", \"Updated itinerary with budget tips...\")\n",
    "print(\"\\nSummary 2:\\n\", s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise B:\n",
    "- Constrain summary length to ~100 tokens.\n",
    "- Compare recall quality vs. brevity across 10+ turns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Vector Memory with Chroma\n",
    "Store semantic memories (preferences, facts) as embeddings and retrieve when needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "persist_dir = \"./chroma_memory\"\n",
    "os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vs = Chroma(collection_name=\"memories\", embedding_function=embeddings, persist_directory=persist_dir)\n",
    "    print(\"Chroma vector store ready.\")\n",
    "except Exception as e:\n",
    "    embeddings = None\n",
    "    vs = None\n",
    "    print(\"Chroma/Embeddings unavailable:\", e)\n",
    "\n",
    "def remember(text: str, metadata: Optional[Dict[str, Any]] = None):\n",
    "    if vs is None:\n",
    "        print(\"Vector store not available; skipping.\")\n",
    "        return\n",
    "    doc = Document(page_content=text, metadata=metadata or {})\n",
    "    vs.add_documents([doc])\n",
    "    vs.persist()\n",
    "\n",
    "def recall(query: str, k: int = 3) -> List[Document]:\n",
    "    if vs is None:\n",
    "        print(\"Vector store not available; returning empty list.\")\n",
    "        return []\n",
    "    retriever = vs.as_retriever(search_kwargs={\"k\": k})\n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "# Demo\n",
    "remember(\"User prefers concise answers\", {\"type\": \"preference\"})\n",
    "remember(\"Project is about LangChain memory\", {\"type\": \"context\"})\n",
    "\n",
    "docs = recall(\"What does the user prefer?\")\n",
    "for d in docs:\n",
    "    print(\"Retrieved:\", d.metadata, \"|\", d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise C:\n",
    "- Insert 5+ conversation facts and build a function `build_context(query)` that retrieves and concatenates top-k facts for prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Knowledge Graph Memory (Triple Store)\n",
    "Lightweight structured memory to store (subject, predicate, object) triples for precise recall."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class TripleStore:\n",
    "    def __init__(self):\n",
    "        self.forward = defaultdict(set)  # (s, p) -> {o}\n",
    "        self.reverse = defaultdict(set)  # (o, p) -> {s}\n",
    "    def add(self, s: str, p: str, o: str):\n",
    "        self.forward[(s, p)].add(o)\n",
    "        self.reverse[(o, p)].add(s)\n",
    "    def objects(self, s: str, p: str) -> List[str]:\n",
    "        return list(self.forward.get((s, p), []))\n",
    "    def subjects(self, o: str, p: str) -> List[str]:\n",
    "        return list(self.reverse.get((o, p), []))\n",
    "\n",
    "kg = TripleStore()\n",
    "kg.add(\"Alice\", \"likes\", \"RAG\")\n",
    "kg.add(\"Alice\", \"role\", \"Engineer\")\n",
    "kg.add(\"ProjectX\", \"uses\", \"LangChain\")\n",
    "\n",
    "print(\"Alice likes:\", kg.objects(\"Alice\", \"likes\"))\n",
    "print(\"Who uses LangChain:\", kg.subjects(\"LangChain\", \"uses\"))\n",
    "\n",
    "def extract_triples_stub(text: str) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"A simple stub that finds 'X likes Y' patterns; replace with LLM extraction for robustness.\"\"\"\n",
    "    triples: List[Tuple[str, str, str]] = []\n",
    "    for m in re.finditer(r\"(\\b[A-Z][a-zA-Z0-9_]+)\\s+likes\\s+(\\b[A-Z][a-zA-Z0-9_]+)\", text):\n",
    "        s, o = m.group(1), m.group(2)\n",
    "        triples.append((s, \"likes\", o))\n",
    "    return triples\n",
    "\n",
    "found = extract_triples_stub(\"Bob likes LangChain. Carol likes RAG.\")\n",
    "for s, p, o in found:\n",
    "    kg.add(s, p, o)\n",
    "print(\"KG after extraction - Bob likes:\", kg.objects(\"Bob\", \"likes\"))\n",
    "print(\"KG after extraction - Carol likes:\", kg.objects(\"Carol\", \"likes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise D:\n",
    "- Implement `extract_triples(text)` using an LLM prompt and parse structured output (e.g., JSON markdown block) to populate the `TripleStore`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Persistence Patterns\n",
    "Combine multiple memory forms and save to disk (JSON for buffer/summary; Chroma for vector memory)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"buffer\": [],\n",
    "    \"summary\": \"\"\n",
    "}\n",
    "\n",
    "def save_state(path: str = \"memory_state.json\"):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(state, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_state(path: str = \"memory_state.json\") -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {\"buffer\": [], \"summary\": \"\"}\n",
    "\n",
    "# Persist an example state\n",
    "state[\"buffer\"].append({\"role\": \"user\", \"content\": \"Hello again\"})\n",
    "state[\"summary\"] = \"User greeted; topic pending.\"\n",
    "save_state()\n",
    "print(\"Saved state:\", load_state())\n",
    "\n",
    "# Note: Chroma vector store persistence already handled via 'persist_directory' and vs.persist() calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise E:\n",
    "- Add simple encryption-at-rest for JSON memory using a symmetric key (e.g., Fernet).\n",
    "- Discuss trade-offs in key management and performance overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Capstone: Memory-Enabled Assistant\n",
    "Combine buffer, summary, KG, and vector memories into a single assistant wrapper."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MemoryAssistant:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "        except Exception as e:\n",
    "            self.llm = None\n",
    "            print(\"LLM unavailable:\", e)\n",
    "\n",
    "        self.buffer = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"input\")\n",
    "        self.summary = \"\"\n",
    "        self.kg = TripleStore()\n",
    "\n",
    "        try:\n",
    "            self.vs = Chroma(\n",
    "                collection_name=\"assistant_mem\",\n",
    "                embedding_function=OpenAIEmbeddings(),\n",
    "                persist_directory=\"./assistant_mem\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.vs = None\n",
    "            print(\"Vector store unavailable:\", e)\n",
    "\n",
    "        # Summarizer\n",
    "        try:\n",
    "            self.summarizer = LLMChain(\n",
    "                llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "                prompt=PromptTemplate(\n",
    "                    template=\"\"\"Summarize:\n",
    "{conversation}\n",
    "Summary:\"\"\",\n",
    "                    input_variables=[\"conversation\"]\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.summarizer = None\n",
    "            print(\"Summarizer chain unavailable:\", e)\n",
    "\n",
    "        self.chat_prompt = PromptTemplate(\n",
    "            template=\"\"\"System: Use prior context when helpful.\n",
    "{chat_history}\n",
    "Facts: {facts}\n",
    "Retrieved: {retrieved}\n",
    "Human: {input}\n",
    "Assistant:\"\"\",\n",
    "            input_variables=[\"chat_history\", \"facts\", \"retrieved\", \"input\"]\n",
    "        )\n",
    "\n",
    "        if self.llm:\n",
    "            self.chat_chain = LLMChain(llm=self.llm, prompt=self.chat_prompt, memory=self.buffer)\n",
    "        else:\n",
    "            self.chat_chain = None\n",
    "\n",
    "    def add_fact(self, s: str, p: str, o: str):\n",
    "        self.kg.add(s, p, o)\n",
    "        if self.vs:\n",
    "            self.vs.add_texts([f\"{s} {p} {o}\"])\n",
    "            self.vs.persist()\n",
    "\n",
    "    def summarize(self) -> str:\n",
    "        if not self.summarizer:\n",
    "            return \"Summarizer unavailable\"\n",
    "        convo = self.buffer.buffer\n",
    "        try:\n",
    "            self.summary = self.summarizer.run(conversation=convo)\n",
    "            return self.summary\n",
    "        except Exception as e:\n",
    "            return f\"Summarize failed: {e}\"\n",
    "\n",
    "    def ask(self, text: str) -> str:\n",
    "        # Retrieve\n",
    "        if self.vs:\n",
    "            try:\n",
    "                retrieved_docs = self.vs.as_retriever(search_kwargs={\"k\": 3}).get_relevant_documents(text)\n",
    "            except Exception as e:\n",
    "                print(\"Retrieval failed:\", e)\n",
    "                retrieved_docs = []\n",
    "        else:\n",
    "            retrieved_docs = []\n",
    "        retrieved = \" \\n\".join([d.page_content for d in retrieved_docs]) if retrieved_docs else \"\"\n",
    "\n",
    "        # Facts (example: list what Alice likes)\n",
    "        facts = \", \".join(self.kg.objects(\"Alice\", \"likes\"))\n",
    "\n",
    "        if not self.chat_chain:\n",
    "            return \"Chat chain unavailable (no LLM).\"\n",
    "        try:\n",
    "            return self.chat_chain.run(input=text, facts=facts, retrieved=retrieved)\n",
    "        except Exception as e:\n",
    "            return f\"Ask failed: {e}\"\n",
    "\n",
    "# Demo\n",
    "assistant = MemoryAssistant()\n",
    "assistant.add_fact(\"Alice\", \"likes\", \"RAG\")\n",
    "resp = assistant.ask(\"What do we know about Alice?\")\n",
    "print(\"Assistant:\", resp if len(resp) < 400 else resp[:400] + \" ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise F (Capstone):\n",
    "- Drop old turns when updating the summary and clear buffer accordingly.\n",
    "- Add per-user namespaces (e.g., separate Chroma collections per user).\n",
    "- Implement `export_state()` that saves buffer, summary, KG triples, and vector store metadata to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "You implemented multiple memory strategies and combined them into a robust assistant. Next, build on this by integrating tools and agents, and later move to LangGraph for workflow orchestration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}