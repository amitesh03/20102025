{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Vector Databases and Retrieval (RAG) - Interactive Notebook\n\nThis notebook provides hands-on examples to accompany the vector retrieval lesson:\n- Generate embeddings and chunk text\n- Build and persist Chroma and FAISS indexes\n- Use retrievers (similarity, MMR, threshold)\n- Hybrid search with BM25 + vectors via RRF\n- Minimal RAG chain with citations\n- Quick evaluation of retrieval quality\n\nEnsure your `.env` has OPENAI_API_KEY for embeddings/LLM, and you have `chromadb` and `faiss-cpu` installed."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0) Setup and Imports"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import os\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport numpy as np\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# LangChain core\nfrom langchain_core.documents import Document\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.chains import LLMChain\n\n# Models & embeddings\ntry:\n    from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n    openai_available = True\nexcept Exception as e:\n    print(\"OpenAI integrations not available:\", e)\n    openai_available = False\n\n# Vector stores\nfrom langchain_community.vectorstores import Chroma, FAISS\n\n# Text splitters\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n\n# BM25 retriever (for hybrid)\ntry:\n    from langchain_community.retrievers import BM25Retriever\n    bm25_available = True\nexcept Exception as e:\n    print(\"BM25 retriever not available:\", e)\n    bm25_available = False\n\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1) Embeddings (OpenAI) - Quick Demo"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "embeddings = None\nif openai_available:\n    try:\n        embeddings = OpenAIEmbeddings()\n        vec = embeddings.embed_query(\"LangChain makes RAG easy\")\n        print(\"Embedding length:\", len(vec), \"sample:\", vec[:5])\n    except Exception as e:\n        print(\"Failed to initialize embeddings (check OPENAI_API_KEY):\", e)\nelse:\n    print(\"OpenAI embeddings unavailable. Continuing with code that does not require embeddings.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2) Chunking Strategies (Recursive and Token-based)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "text = (\n    \"LangChain is a framework for developing applications powered by LLMs. \"\n    \"It provides prompts, chains, agents, memory, and integrations. \"\n    \"Vector databases enable semantic search via embeddings.\"\n)\n\n# Recursive splitter\nrecursive = RecursiveCharacterTextSplitter(\n    chunk_size=400, chunk_overlap=60, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\nrec_chunks = recursive.split_text(text)\nprint(\"Recursive chunks:\", len(rec_chunks))\nfor i, ch in enumerate(rec_chunks):\n    print(f\"{i+1}: {len(ch)} chars\")\n\n# Token-based splitter\ntry:\n    token_splitter = TokenTextSplitter(chunk_size=80, chunk_overlap=20, encoding_name=\"cl100k_base\")\n    tok_chunks = token_splitter.split_text(text)\n    print(\"Token chunks:\", len(tok_chunks))\n    for i, ch in enumerate(tok_chunks):\n        print(f\"{i+1}: {len(ch)} chars\")\nexcept Exception as e:\n    print(\"TokenTextSplitter unavailable:\", e)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3) Build a Persistent Chroma Index"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "persist_dir = \"./chroma_rag\"\nPath(persist_dir).mkdir(exist_ok=True)\n\n# Sample docs\ndocs = [\n    Document(page_content=\"LangChain supports prompts, chains, agents, and memory.\",\n             metadata={\"source\": \"notes\", \"topic\": \"langchain\"}),\n    Document(page_content=\"Vector databases enable semantic search via embeddings.\",\n             metadata={\"source\": \"notes\", \"topic\": \"vectors\"}),\n    Document(page_content=\"RAG combines retrieval with generation to ground responses in data.\",\n             metadata={\"source\": \"notes\", \"topic\": \"rag\"}),\n]\n\nvectordb = None\nif embeddings is not None:\n    try:\n        vectordb = Chroma(\n            collection_name=\"demo\",\n            embedding_function=embeddings,\n            persist_directory=persist_dir\n        )\n        vectordb.add_documents(docs)\n        vectordb.persist()\n        try:\n            cnt = vectordb._collection.count()\n            print(\"Chroma collection count:\", cnt)\n        except Exception:\n            print(\"Chroma persisted.\")\n    except Exception as e:\n        print(\"Chroma error:\", e)\nelse:\n    print(\"Skipping Chroma build (no embeddings).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Metadata Filtering with Chroma (if available)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "if vectordb is not None:\n    try:\n        results = vectordb.similarity_search(\"semantic search\", k=3, filter={\"topic\": \"vectors\"})\n        print(\"Filtered results:\")\n        for d in results:\n            print(d.metadata, \"|\", d.page_content[:60])\n    except Exception as e:\n        print(\"Filtering error:\", e)\nelse:\n    print(\"Vectordb not initialized.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4) FAISS: Build, Search, and Persist/Load (In-memory index)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "faiss_db = None\nif embeddings is not None:\n    try:\n        faiss_db = FAISS.from_documents(docs, embeddings)\n        hits = faiss_db.similarity_search(\"What enables semantic search?\", k=2)\n        print(\"FAISS hits:\")\n        for h in hits:\n            print(h.metadata, \"|\", h.page_content[:60])\n        # Persist and load\n        faiss_db.save_local(\"./faiss_demo\")\n        faiss_loaded = FAISS.load_local(\"./faiss_demo\", embeddings, allow_dangerous_deserialization=True)\n        print(\"FAISS loaded; test query count:\", len(faiss_loaded.similarity_search(\"semantic\", k=2)))\n    except Exception as e:\n        print(\"FAISS error:\", e)\nelse:\n    print(\"Skipping FAISS (no embeddings).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5) Retriever Strategies: Similarity, MMR, Threshold (Chroma)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "sim_retriever = None\nmmr_retriever = None\nthr_retriever = None\n\nif vectordb is not None:\n    try:\n        sim_retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n        sim_docs = sim_retriever.get_relevant_documents(\"What does LangChain provide?\")\n        print(\"Similarity retriever results:\")\n        for d in sim_docs:\n            print(\"-\", d.page_content)\n    except Exception as e:\n        print(\"Similarity retriever error:\", e)\n\n    try:\n        mmr_retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5, \"lambda_mult\": 0.5})\n        mmr_docs = mmr_retriever.get_relevant_documents(\"semantic search and vectors\")\n        print(\"\\nMMR retriever results:\")\n        for d in mmr_docs:\n            print(\"-\", d.page_content)\n    except Exception as e:\n        print(\"MMR retriever error:\", e)\n\n    try:\n        thr_retriever = vectordb.as_retriever(\n            search_type=\"similarity_score_threshold\",\n            search_kwargs={\"score_threshold\": 0.2, \"k\": 8}\n        )\n        thr_docs = thr_retriever.get_relevant_documents(\"obscure topic that might not exist\")\n        print(\"\\nThreshold retriever results (may be empty):\", len(thr_docs))\n    except Exception as e:\n        print(\"Threshold retriever error:\", e)\nelse:\n    print(\"Chroma not available; skipping retriever demos.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6) Hybrid Search: BM25 + Vectors via RRF (Reciprocal Rank Fusion)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def rrf(ranks: List[int], k: int = 60) -> float:\n    return sum(1.0 / (k + r) for r in ranks)\n\nhybrid_enabled = bm25_available and (vectordb is not None)\n\nif hybrid_enabled:\n    # Build BM25 from our docs\n    bm25 = BM25Retriever.from_texts([d.page_content for d in docs])\n    bm25.k = 4\n\n    def hybrid_search(query: str, top_k: int = 5):\n        bm25_hits = bm25.get_relevant_documents(query)\n        vec_hits = vectordb.similarity_search_with_score(query, k=top_k)\n\n        rank_map = defaultdict(list)\n        for i, d in enumerate(bm25_hits):\n            rank_map[hash(d.page_content)].append(i + 1)\n        for j, (d, _) in enumerate(vec_hits):\n            rank_map[hash(d.page_content)].append(j + 1)\n\n        scored = []\n        for key, ranks in rank_map.items():\n            scored.append((rrf(ranks), key))\n        scored.sort(reverse=True)\n\n        # Recover original docs\n        needle = {hash(d.page_content): d for d in [*bm25_hits, *[d for d, _ in vec_hits]]}\n        return [needle[h] for _, h in scored[:top_k]]\n\n    combo = hybrid_search(\"semantic search\")\n    print(\"Hybrid search results:\")\n    for d in combo:\n        print(\"-\", d.page_content)\nelse:\n    print(\"Hybrid search disabled (BM25 or Chroma missing).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7) Minimal RAG Chain with Citations (if LLM available)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "llm = None\nif openai_available:\n    try:\n        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n    except Exception as e:\n        print(\"ChatOpenAI unavailable:\", e)\n\nrag_prompt = PromptTemplate(\n    template=(\n        \"\"\"You are a helpful assistant. Answer only using the provided context. \nIf the answer is not in the context, say 'I don't know'.\n\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n    ),\n    input_variables=[\"context\", \"question\"],\n)\n\ndef answer(question: str, retriever=sim_retriever) -> Dict[str, Any]:\n    if retriever is None:\n        return {\"error\": \"retriever not available\"}\n    ctx_docs = retriever.get_relevant_documents(question)\n    context = \"\\n\\n\".join(d.page_content for d in ctx_docs)\n    if llm is None:\n        return {\"context\": context, \"note\": \"LLM unavailable; showing retrieved context only.\"}\n    chain = LLMChain(llm=llm, prompt=rag_prompt)\n    out = chain.run(context=context, question=question)\n    return {\"answer\": out, \"citations\": [d.metadata for d in ctx_docs]}\n\nprint(answer(\"What components does LangChain provide?\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8) Quick Retrieval Evaluation (Hit Rate@k)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "gold = {\n    \"What is LangChain?\": \"LangChain supports prompts, chains, agents, and memory.\",\n    \"How do we search semantically?\": \"Vector databases enable semantic search via embeddings.\",\n}\n\ndef hit_rate(retriever, k: int = 3) -> float:\n    if retriever is None:\n        return 0.0\n    hits = 0\n    for q, must_have in gold.items():\n        docs = retriever.get_relevant_documents(q)\n        snapshot = \"\\n\".join(d.page_content for d in docs[:k])\n        if must_have.lower() in snapshot.lower():\n            hits += 1\n    return hits / len(gold)\n\nprint(\"Similarity@3:\", hit_rate(sim_retriever, k=3))\nprint(\"MMR@3:\", hit_rate(mmr_retriever, k=3))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9) Notes on Persistence and Freshness\n- Chroma: call `persist()` after writes; reopen with the same `persist_directory`.\n- FAISS: use `save_local` and `load_local`.\n- Use stable document IDs and upsert semantics to avoid duplicates.\n- Track content hashes to detect when to rebuild chunks.\n- Keep chunk sizes between ~200–1000 tokens; use overlaps when necessary."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}