{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LangGraph Workflows and State Machines - Interactive Notebook\n\nThis notebook provides hands-on examples to accompany the lesson in [`09_langgraph_workflows.md`](09_langgraph_workflows.md):\n- Define state, nodes, and edges\n- Conditional routing and loops\n- Streaming execution events\n- Persistence with a checkpointer\n- LLM/tool integration inside nodes\n- Error handling and retries\n\nEnsure your `.env` has OPENAI_API_KEY for LLM examples."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0) Setup and Imports"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import os\nfrom typing import TypedDict, Optional, List, Dict, Any\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# LangGraph core\nfrom langgraph.graph import StateGraph, START, END\n\n# Optional: LLM for node integration\ntry:\n    from langchain_openai import ChatOpenAI\n    from langchain_core.prompts import ChatPromptTemplate\n    llm_available = True\nexcept Exception as e:\n    print(\"LLM integrations not available:\", e)\n    llm_available = False\n\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1) Minimal Linear Flow: State, Nodes, Edges"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class AppState(TypedDict, total=False):\n    input_text: str\n    processed: str\n\n# Node: normalize input\ndef normalize(state: AppState) -> AppState:\n    text = state.get(\"input_text\", \"\")\n    return {\"processed\": text.strip().lower()}\n\n# Node: finalize (pass-through)\ndef finalize(state: AppState) -> AppState:\n    return state\n\n# Build and compile graph\nbuilder = StateGraph(AppState)\nbuilder.add_node(\"normalize\", normalize)\nbuilder.add_node(\"finalize\", finalize)\nbuilder.add_edge(START, \"normalize\")\nbuilder.add_edge(\"normalize\", \"finalize\")\nbuilder.add_edge(\"finalize\", END)\nlinear_graph = builder.compile()\n\n# Invoke\nout = linear_graph.invoke({\"input_text\": \"  Hello LangGraph  \"})\nprint(\"Linear output:\", out)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2) Streaming Execution Events"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "print(\"Streaming events:\")\nfor event in linear_graph.astream_events({\"input_text\": \"  Hi  \"}):\n    print(event.get(\"event\"), event.get(\"name\"), event.get(\"timestamp\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3) Conditional Routing"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def is_long(state: AppState) -> str:\n    text = state.get(\"input_text\", \"\")\n    return \"long\" if len(text) > 20 else \"short\"\n\ndef handle_long(state: AppState) -> AppState:\n    return {\"processed\": f\"LONG::{state.get('input_text','')}\"}\n\ndef handle_short(state: AppState) -> AppState:\n    return {\"processed\": f\"SHORT::{state.get('input_text','')}\"}\n\nrouter = StateGraph(AppState)\nrouter.add_node(\"handle_long\", handle_long)\nrouter.add_node(\"handle_short\", handle_short)\nrouter.add_conditional_edges(START, is_long, {\"long\": \"handle_long\", \"short\": \"handle_short\"})\nrouter.add_edge(\"handle_long\", END)\nrouter.add_edge(\"handle_short\", END)\nconditional_graph = router.compile()\n\nprint(conditional_graph.invoke({\"input_text\": \"tiny\"}))\nprint(conditional_graph.invoke({\"input_text\": \"this string is definitely long\"}))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4) Loops with a Counter in State"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class LoopState(TypedDict, total=False):\n    step: int\n    log: List[str]\n\ndef stepper(state: LoopState) -> LoopState:\n    n = state.get(\"step\", 0) + 1\n    log = (state.get(\"log\") or []) + [f\"step {n}\"]\n    return {\"step\": n, \"log\": log}\n\nloop_builder = StateGraph(LoopState)\nloop_builder.add_node(\"stepper\", stepper)\nloop_builder.add_edge(START, \"stepper\")\n\n# Router to loop until step < 3\ndef check(state: LoopState) -> str:\n    return \"again\" if state.get(\"step\", 0) < 3 else \"done\"\n\nloop_builder.add_conditional_edges(\"stepper\", check, {\"again\": \"stepper\", \"done\": END})\nloop_graph = loop_builder.compile()\n\nprint(loop_graph.invoke({}))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5) Integrate LLM Calls Inside Nodes (Optional)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "llm_graph = None\nif llm_available:\n    try:\n        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n        def summarize(state: AppState) -> AppState:\n            text = state.get(\"input_text\", \"\")\n            if not text:\n                return {}\n            prompt = ChatPromptTemplate.from_messages([\n                (\"system\", \"Summarize concisely\"),\n                (\"human\", \"{text}\")\n            ])\n            messages = prompt.format_messages(text=text)\n            resp = llm.invoke(messages)\n            return {\"processed\": resp.content}\n        llm_builder = StateGraph(AppState)\n        llm_builder.add_node(\"summarize\", summarize)\n        llm_builder.add_edge(START, \"summarize\")\n        llm_builder.add_edge(\"summarize\", END)\n        llm_graph = llm_builder.compile()\n        print(llm_graph.invoke({\"input_text\": \"LangGraph builds stateful LLM workflows.\"}))\n    except Exception as e:\n        print(\"LLM node failed:\", e)\nelse:\n    print(\"LLM unavailable; skipping summarize node demo.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6) Persistence with a Checkpointer (SQLite)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "try:\n    from langgraph.checkpoint.sqlite import SqliteSaver\n    checkpointer = SqliteSaver.from_conn_string(\"sqlite:///langgraph_state.db\")\n\n    # Reuse the linear builder for persistence demo\n    persistent_graph = builder.compile(checkpointer=checkpointer)\n\n    config = {\"configurable\": {\"thread_id\": \"user-123\"}}\n    print(\"Persist invoke 1:\", persistent_graph.invoke({\"input_text\": \"hello\"}, config=config))\n    print(\"Persist invoke 2:\", persistent_graph.invoke({\"input_text\": \"continue\"}, config=config))\nexcept Exception as e:\n    print(\"Checkpointer unavailable:\", e)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7) Error Handling and Conditional Error Routing"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def robust(state: AppState) -> AppState:\n    try:\n        assert \"input_text\" in state, \"missing input_text\"\n        return {\"processed\": state[\"input_text\"].upper()}\n    except Exception as e:\n        return {\"error\": str(e)}\n\ndef route_errors(state: AppState) -> str:\n    return \"err\" if \"error\" in state else \"ok\"\n\nerr_builder = StateGraph(AppState)\nerr_builder.add_node(\"robust\", robust)\nerr_builder.add_node(\"on_ok\", lambda s: s)\nerr_builder.add_node(\"on_err\", lambda s: s)\nerr_builder.add_edge(START, \"robust\")\nerr_builder.add_conditional_edges(\"robust\", route_errors, {\"ok\": \"on_ok\", \"err\": \"on_err\"})\nerr_builder.add_edge(\"on_ok\", END)\nerr_builder.add_edge(\"on_err\", END)\nerr_graph = err_builder.compile()\n\nprint(\"Error path (missing input):\", err_graph.invoke({}))\nprint(\"OK path:\", err_graph.invoke({\"input_text\": \"ok\"}))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8) Streaming Observability (Async Skeleton)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import asyncio\n\nasync def run_streaming_demo():\n    async for ev in linear_graph.astream_events({\"input_text\": \"trace me\"}):\n        print(ev.get(\"event\"), ev.get(\"name\"), ev.get(\"timestamp\"))\n\n# Uncomment to run async streaming\n# asyncio.run(run_streaming_demo())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9) Capstone Skeleton: Branching Summarize-or-Classify with Loop"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class PipelineState(TypedDict, total=False):\n    text: str\n    mode: str\n    summary: str\n    label: str\n    confirm: bool\n\n# Router: choose summarize vs classify based on word count\ndef choose(state: PipelineState) -> str:\n    words = len((state.get(\"text\") or \"\").split())\n    return \"summarize\" if words > 12 else \"classify\"\n\n# Classify node (simple heuristic)\ndef classify(state: PipelineState) -> PipelineState:\n    txt = (state.get(\"text\") or \"\").lower()\n    label = \"tech\" if (\"langgraph\" in txt or \"llm\" in txt) else \"other\"\n    return {\"label\": label}\n\n# Summarize node (stub; integrate LLM if available)\ndef summarize_node(state: PipelineState) -> PipelineState:\n    t = (state.get(\"text\") or \"\")\n    if llm_available:\n        try:\n            llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n            prompt = ChatPromptTemplate.from_messages([\n                (\"system\", \"Summarize succinctly\"),\n                (\"human\", \"{text}\")\n            ])\n            messages = prompt.format_messages(text=t)\n            resp = llm.invoke(messages)\n            return {\"summary\": resp.content}\n        except Exception as e:\n            return {\"summary\": f\"Failed to summarize: {e}\"}\n    return {\"summary\": (t[:50] + \"...\") if t else \"\"}\n\n# Loop router: continue until confirm=True\ndef should_continue(state: PipelineState) -> str:\n    return \"again\" if not state.get(\"confirm\") else \"done\"\n\nb = StateGraph(PipelineState)\nb.add_node(\"classify\", classify)\nb.add_node(\"summarize\", summarize_node)\nb.add_conditional_edges(START, choose, {\"summarize\": \"summarize\", \"classify\": \"classify\"})\nb.add_conditional_edges(\"classify\", should_continue, {\"again\": \"classify\", \"done\": END})\nb.add_conditional_edges(\"summarize\", should_continue, {\"again\": \"summarize\", \"done\": END})\ncapstone = b.compile()\n\nprint(capstone.invoke({\"text\": \"LangGraph builds stateful LLM apps for production\", \"confirm\": True}))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\nYou built LangGraph workflows with nodes/edges, conditional routing, loops, streaming events, persistence, LLM integration, and error handling. Extend these patterns to tool-calling nodes and RAG-based nodes as needed."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}