{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performance Optimization and Monitoring\n",
        "\n",
        "This notebook complements `12_performance_optimization.md` with runnable code for baselining, caching, routing, concurrency, profiling, and monitoring in LLM applications.\n",
        "\n",
        "Notes:\n",
        "- Some cells use optional libraries (e.g., `tiktoken`, `langchain_openai`, `langgraph`). Guarded imports are used so the notebook remains runnable even if those packages are absent.\n",
        "- Replace stubs or mocks with live integrations when you have credentials and dependencies configured.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Setup and Utilities\n",
        "Imports and environment checks for optional dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup: imports and optional dependencies\n",
        "import time\n",
        "import json\n",
        "import asyncio\n",
        "import random\n",
        "import hashlib\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "try:\n",
        "    import tiktoken\n",
        "except Exception:\n",
        "    tiktoken = None\n",
        "\n",
        "try:\n",
        "    from langchain_core.prompts import PromptTemplate\n",
        "except Exception:\n",
        "    PromptTemplate = None\n",
        "\n",
        "try:\n",
        "    from langchain_openai import ChatOpenAI\n",
        "except Exception:\n",
        "    ChatOpenAI = None\n",
        "\n",
        "try:\n",
        "    from langgraph.graph import StateGraph, START, END\n",
        "except Exception:\n",
        "    StateGraph = None\n",
        "    START = None\n",
        "    END = None\n",
        "\n",
        "print({\n",
        "    'tiktoken': bool(tiktoken),\n",
        "    'PromptTemplate': bool(PromptTemplate),\n",
        "    'ChatOpenAI': bool(ChatOpenAI),\n",
        "    'LangGraph': bool(StateGraph)\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Baseline Metrics\n",
        "A minimal measurement harness for latency and tokens per request."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
        "    \"\"\"Count tokens with tiktoken if available; fallback to naive split.\"\"\"\n",
        "    if tiktoken is not None:\n",
        "        try:\n",
        "            enc = tiktoken.encoding_for_model(model)\n",
        "        except KeyError:\n",
        "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        return len(enc.encode(text))\n",
        "    # naive fallback\n",
        "    return max(1, len(text.strip().split()))\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.errors = 0\n",
        "        self.total_time = 0.0\n",
        "        self._times = []\n",
        "        self.total_tokens = 0\n",
        "\n",
        "    def record(self, start: float, in_tokens: int, out_tokens: int, error: bool = False):\n",
        "        self.requests += 1\n",
        "        self.errors += int(error)\n",
        "        dt = (time.time() - start)\n",
        "        self.total_time += dt\n",
        "        self._times.append(dt)\n",
        "        self.total_tokens += (in_tokens + out_tokens)\n",
        "\n",
        "    def summary(self) -> Dict[str, Any]:\n",
        "        times = sorted(self._times)\n",
        "        p50 = times[int(0.5 * (len(times)-1))] * 1000 if times else 0\n",
        "        p95 = times[int(0.95 * (len(times)-1))] * 1000 if times else 0\n",
        "        return {\n",
        "            \"requests\": self.requests,\n",
        "            \"errors\": self.errors,\n",
        "            \"avg_ms\": (self.total_time / max(1, self.requests)) * 1000,\n",
        "            \"p50_ms\": p50,\n",
        "            \"p95_ms\": p95,\n",
        "            \"avg_tokens\": self.total_tokens / max(1, self.requests),\n",
        "        }\n",
        "\n",
        "# Demo: simulate 10 requests with random latencies\n",
        "m = Metrics()\n",
        "for _ in range(10):\n",
        "    t0 = time.time()\n",
        "    time.sleep(random.uniform(0.01, 0.05))\n",
        "    in_t = random.randint(10, 60)\n",
        "    out_t = random.randint(20, 120)\n",
        "    m.record(t0, in_t, out_t, error=False)\n",
        "m.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Prompt and Context Optimization\n",
        "Refactor templates to keep outputs concise and bound context size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if PromptTemplate is not None:\n",
        "    rag_prompt = PromptTemplate(\n",
        "        template=(\n",
        "            \"Answer using only the context. If missing, say 'I don't know'.\\n\"\n",
        "            \"Context:\\n{context}\\n\\n\"\n",
        "            \"Question: {question}\\n\"\n",
        "            \"Answer (concise, 2-3 sentences):\"\n",
        "        ),\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "    )\n",
        "    print(\"PromptTemplate ready\")\n",
        "else:\n",
        "    print(\"PromptTemplate unavailable; skipping\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Token Budgeting and Early Exit\n",
        "Pre-truncate long inputs by token limits using `tiktoken` when present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def truncate_by_tokens(text: str, max_tokens: int = 200, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    if tiktoken is not None:\n",
        "        try:\n",
        "            enc = tiktoken.encoding_for_model(model)\n",
        "        except KeyError:\n",
        "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        toks = enc.encode(text)\n",
        "        if len(toks) <= max_tokens:\n",
        "            return text\n",
        "        return enc.decode(toks[:max_tokens])\n",
        "    # naive fallback: split words\n",
        "    words = text.split()\n",
        "    return \" \".join(words[:max_tokens])\n",
        "\n",
        "sample = \"Lorem ipsum \" * 100\n",
        "len(sample.split()), len(truncate_by_tokens(sample, max_tokens=50).split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Batching and Concurrency\n",
        "Use asyncio with a semaphore to bound concurrency; add backoff/retry for transient errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "from asyncio import Semaphore\n",
        "\n",
        "class RateLimitError(Exception):\n",
        "    pass\n",
        "\n",
        "async def call_model(prompt: str, max_latency_ms: int = 50) -> str:\n",
        "    # Simulate latency and occasional rate limit\n",
        "    await asyncio.sleep(random.uniform(0.005, max_latency_ms/1000))\n",
        "    if random.random() < 0.05:\n",
        "        raise RateLimitError(\"429: Too Many Requests\")\n",
        "    return f\"out:{prompt[:20]}\"\n",
        "\n",
        "async def backoff_retry(fn, *args, retries: int = 3, base_ms: int = 50, **kwargs):\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            return await fn(*args, **kwargs)\n",
        "        except RateLimitError as e:\n",
        "            await asyncio.sleep((base_ms * (2 ** i)) / 1000)\n",
        "    return await fn(*args, **kwargs)\n",
        "\n",
        "async def run_batch(prompts: List[str], max_concurrency: int = 5) -> List[str]:\n",
        "    sem = Semaphore(max_concurrency)\n",
        "    async def worker(p):\n",
        "        async with sem:\n",
        "            return await backoff_retry(call_model, p)\n",
        "    return await asyncio.gather(*(worker(p) for p in prompts))\n",
        "\n",
        "# Demo\n",
        "res = asyncio.run(run_batch([f\"q{i}\" for i in range(20)], max_concurrency=8))\n",
        "len(res), res[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Caching Layers\n",
        "Prompt/response cache keyed by normalized payload."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ResponseCache:\n",
        "    def __init__(self):\n",
        "        self.store = {}\n",
        "\n",
        "    def key(self, prompt: str, **params) -> str:\n",
        "        payload = json.dumps({\"prompt\": prompt, **params}, sort_keys=True)\n",
        "        return hashlib.md5(payload.encode()).hexdigest()\n",
        "\n",
        "    def get(self, prompt: str, **params):\n",
        "        return self.store.get(self.key(prompt, **params))\n",
        "\n",
        "    def set(self, prompt: str, value: str, **params):\n",
        "        self.store[self.key(prompt, **params)] = value\n",
        "\n",
        "# Demo\n",
        "cache = ResponseCache()\n",
        "p = \"What is MMR retrieval?\"\n",
        "kparams = {\"model\":\"gpt-3.5-turbo\", \"temperature\":0}\n",
        "print(cache.get(p, **kparams))\n",
        "cache.set(p, \"MMR balances relevance and diversity.\", **kparams)\n",
        "print(cache.get(p, **kparams))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Model Routing (Smart LLM Selection)\n",
        "Route easy tasks to smaller models; complex tasks to larger ones. Uses `langchain_openai.ChatOpenAI` if available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Router:\n",
        "    def __init__(self):\n",
        "        self.simple = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0) if ChatOpenAI else None\n",
        "        self.complex = ChatOpenAI(model=\"gpt-4\", temperature=0) if ChatOpenAI else None\n",
        "\n",
        "    def is_complex(self, text: str) -> bool:\n",
        "        long = len(text.split()) > 60\n",
        "        hard_kw = any(k in text.lower() for k in [\"analyze\", \"compare\", \"synthesize\", \"proof\", \"derive\"])\n",
        "        return long or hard_kw\n",
        "\n",
        "    def choose(self, text: str):\n",
        "        return self.complex if self.is_complex(text) else self.simple\n",
        "\n",
        "# Demo (without calling the API)\n",
        "router = Router()\n",
        "router.is_complex(\"Summarize this short sentence\") , router.is_complex(\"Analyze and compare approaches to distributed tracing across microservices with proofs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Retrieval Efficiency (MMR)\n",
        "Demonstration with simulated documents. Replace with actual vector DB retriever when available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulated docs and naive MMR-like selection\n",
        "docs = [\n",
        "    {\"content\": \"Vector search uses embeddings for semantic similarity.\", \"vec\": [0.1, 0.9]},\n",
        "    {\"content\": \"BM25 is a keyword-based relevance model.\", \"vec\": [0.8, 0.2]},\n",
        "    {\"content\": \"MMR balances relevance and diversity among candidates.\", \"vec\": [0.5, 0.5]},\n",
        "    {\"content\": \"RRF is a rank fusion technique combining lists.\", \"vec\": [0.6, 0.4]},\n",
        "    {\"content\": \"Hybrid search blends BM25 and vector results.\", \"vec\": [0.7, 0.3]},\n",
        "]\n",
        "\n",
        "def cosine(a, b):\n",
        "    import math\n",
        "    num = sum(x*y for x,y in zip(a,b))\n",
        "    da = math.sqrt(sum(x*x for x in a))\n",
        "    db = math.sqrt(sum(y*y for y in b))\n",
        "    return num / (da*db + 1e-9)\n",
        "\n",
        "qvec = [0.45, 0.55]\n",
        "scores = [(i, cosine(d[\"vec\"], qvec)) for i, d in enumerate(docs)]\n",
        "scores_sorted = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "top = [docs[i][\"content\"] for i,_ in scores_sorted[:3]]\n",
        "\"\\n\\n\".join(s[:80] for s in top)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Profiling Hot Paths\n",
        "Use a simple decorator to measure wall-clock times of functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def timed(fn):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        t0 = time.perf_counter()\n",
        "        out = fn(*args, **kwargs)\n",
        "        dt = (time.perf_counter() - t0) * 1000\n",
        "        print(f\"[timed] {fn.__name__} took {dt:.1f} ms\")\n",
        "        return out\n",
        "    return wrapper\n",
        "\n",
        "@timed\n",
        "def slow_op(n=20000):\n",
        "    s = 0\n",
        "    for i in range(n):\n",
        "        s += (i % 7)\n",
        "    return s\n",
        "\n",
        "slow_op(50000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Monitoring Counters\n",
        "Track tokens, requests, and errors at a coarse level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Counters:\n",
        "    def __init__(self):\n",
        "        self.token_in = 0\n",
        "        self.token_out = 0\n",
        "        self.requests = 0\n",
        "        self.errors = 0\n",
        "\n",
        "    def log_req(self, in_t: int, out_t: int, ok: bool = True):\n",
        "        self.requests += 1\n",
        "        self.token_in += in_t\n",
        "        self.token_out += out_t\n",
        "        if not ok:\n",
        "            self.errors += 1\n",
        "\n",
        "ct = Counters()\n",
        "ct.log_req(50, 120, ok=True)\n",
        "ct.log_req(30, 90, ok=False)\n",
        "{'requests': ct.requests, 'errors': ct.errors, 'token_in': ct.token_in, 'token_out': ct.token_out}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Reliability Patterns: Circuit Breaker\n",
        "Short-circuit repeated errors for a cooldown period to protect upstreams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class CircuitBreaker:\n",
        "    def __init__(self, threshold: int = 3, cooldown: float = 1.0):\n",
        "        self.failures = 0\n",
        "        self.open_until = 0.0\n",
        "        self.threshold = threshold\n",
        "        self.cooldown = cooldown\n",
        "\n",
        "    def call(self, fn, *args, **kwargs):\n",
        "        now = time.time()\n",
        "        if now < self.open_until:\n",
        "            return {\"error\": \"circuit-open\"}\n",
        "        try:\n",
        "            res = fn(*args, **kwargs)\n",
        "            self.failures = 0\n",
        "            return res\n",
        "        except Exception as e:\n",
        "            self.failures += 1\n",
        "            if self.failures >= self.threshold:\n",
        "                self.open_until = now + self.cooldown\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "def flaky(x):\n",
        "    if random.random() < 0.6:\n",
        "        raise RuntimeError(\"boom\")\n",
        "    return x * 2\n",
        "\n",
        "cb = CircuitBreaker(threshold=2, cooldown=1.5)\n",
        "outs = []\n",
        "for _ in range(6):\n",
        "    outs.append(cb.call(flaky, 3))\n",
        "outs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) A/B Testing Scaffold\n",
        "Switch between two prompt variants and measure differences in outcome metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def choose_variant() -> str:\n",
        "    return \"A\" if random.random() < 0.5 else \"B\"\n",
        "\n",
        "def build_prompt(variant: str, context: str, question: str) -> str:\n",
        "    if variant == \"A\":\n",
        "        return f\"Answer using only context:\\n{context}\\nQ: {question}\\nA:\"\n",
        "    else:\n",
        "        return f\"Use context below. If unknown, say 'I don't know'.\\n{context}\\nQ: {question}\\nA (concise):\"\n",
        "\n",
        "v = choose_variant()\n",
        "build_prompt(v, \"ctx: MMR and BM25\", \"Explain hybrid search\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Mini LLM Service (Cache + Metrics)\n",
        "A service wrapper that counts tokens, uses a cache, and logs counters.\n",
        "Uses a mock model to avoid external API calls; replace with `ChatOpenAI` as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MockModel:\n",
        "    def __init__(self, model_name: str = \"mock-model\"):\n",
        "        self.model_name = model_name\n",
        "    def invoke(self, prompt: str, **params):\n",
        "        # Simple transformation to simulate output\n",
        "        return {\"content\": prompt[::-1][:80]}\n",
        "\n",
        "class LLMService:\n",
        "    def __init__(self, model, cache: ResponseCache = None, counters: Counters = None):\n",
        "        self.model = model\n",
        "        self.cache = cache or ResponseCache()\n",
        "        self.counters = counters or Counters()\n",
        "\n",
        "    def invoke(self, prompt: str, **params):\n",
        "        start = time.time()\n",
        "        model_name = getattr(self.model, \"model_name\", \"gpt-3.5-turbo\")\n",
        "        in_toks = count_tokens(prompt, model_name)\n",
        "        cached = self.cache.get(prompt, **params)\n",
        "        if cached:\n",
        "            self.counters.log_req(in_toks, 0, ok=True)\n",
        "            return cached\n",
        "        try:\n",
        "            resp = self.model.invoke(prompt, **params)\n",
        "            out_text = resp[\"content\"] if isinstance(resp, dict) else str(resp)\n",
        "            out_toks = count_tokens(out_text, model_name)\n",
        "            self.counters.log_req(in_toks, out_toks, ok=True)\n",
        "            self.cache.set(prompt, out_text, **params)\n",
        "            return out_text\n",
        "        except Exception as e:\n",
        "            self.counters.log_req(in_toks, 0, ok=False)\n",
        "            return f\"error: {e}\"\n",
        "\n",
        "svc = LLMService(MockModel())\n",
        "out1 = svc.invoke(\"Explain RAG briefly.\", temperature=0)\n",
        "out2 = svc.invoke(\"Explain RAG briefly.\", temperature=0)  # cached\n",
        "svc.counters.__dict__, out1, out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Optional: LangGraph Events Streaming\n",
        "If `langgraph` is installed and you have a graph, you can stream events. Here we construct a tiny mock graph when unavailable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if StateGraph is not None:\n",
        "    from typing import TypedDict\n",
        "    class State(TypedDict):\n",
        "        input_text: str\n",
        "\n",
        "    def node_echo(state: Dict[str, str]):\n",
        "        return {\"input_text\": state[\"input_text\"] + \"_processed\"}\n",
        "\n",
        "    g = StateGraph(State)\n",
        "    g.add_node(\"echo\", node_echo)\n",
        "    g.add_edge(START, \"echo\")\n",
        "    g.add_edge(\"echo\", END)\n",
        "    graph = g.compile()\n",
        "    for ev in graph.astream_events({\"input_text\": \"trace-me\"}):\n",
        "        print(ev.get(\"event\"), ev.get(\"name\"), ev.get(\"timestamp\"))\n",
        "else:\n",
        "    print(\"LangGraph unavailable; skipping events demo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "A) End-to-end metrics\n",
        "- Wrap your main chain/agent with `Metrics` and `Counters`.\n",
        "- Run 50 requests (mock or real) and compute avg/p50/p95 latency and avg tokens.\n",
        "\n",
        "B) Batch embeddings\n",
        "- Write a function that takes a list of texts, hashes them, and computes only missing vectors.\n",
        "- Measure throughput (texts/sec) and cache hit rate on a repeated dataset.\n",
        "\n",
        "C) Router tuning\n",
        "- Implement a 2-tier router (fast vs accurate) and measure cost vs quality on an eval set.\n",
        "\n",
        "D) Cache strategy\n",
        "- Replace `ResponseCache` with Redis (or similar) and set TTL.\n",
        "- Measure cache hit-ratio improvements on repeated queries.\n",
        "\n",
        "E) Tracing integration\n",
        "- Stream LangGraph events to a log with timestamps.\n",
        "- Build a simple Gantt-like timeline from the recorded events."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}